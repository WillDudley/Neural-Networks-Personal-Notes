{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were to ask an ameteur coder to code a tic-tac-toe AI, they'd probably end up with a bunch of if statements relating to the rules of the game. This is against the ethos of reinforcement learning, as we want to be general and not be constrained by rules. For this reason, we also try to avoid giving the agent strategies/tips for succeeding (ie. take opponent's pieces in chess).\n",
    "\n",
    "In reinforcement learning, we need a model for how the game works, we need to know what state yields the highest reward, and we need to know how to transition from one state to the next.\n",
    "\n",
    "Some RL algorithms require us to have a model/environment (like tic-tac-toe). Others don't require any knowledge, and gather their knowledge purely through exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Notation and Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S(t), A(t) \\rightarrow R(t+1),S(t+1)$$\n",
    "is sometimes represented as the tuple\n",
    "$$(s,a,r,s'),$$\n",
    "where $s'$ is the state we go to by doing action $a$ from state $s$, and $r$ is the reward we get by doing action $a$ in state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An \"episode\" is a single run of the game. The type of RL we'll focus on (episodic tasks) learns over many episodes (rather than continuous tasks, which never end).\n",
    "\n",
    "An environment/game can either have discrete states (eg. a board game) or continuous states (eg. the cart pole game). The terminal state is the state where an end condition of the game has been achieved (eg. someone wins or board is full). We will focus on discrete state spaces in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with training a pet, we want to reward good behaviour and maybe even punish bad behaviour. If we were to train a program to solve a maze, we'd want to reward finding the exit (with a 1), and punish every action, including standing still (with a negative number).\n",
    "\n",
    "As mentioned previously, it's bad to \"give tips\" by assigning rewards to subgoals, otherwise the agent may find a way to optimise the net reward based on the subgoals rather than focussing on the overall objective. In short, the agent should be told <i>what</i>, not <i>how</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality we often struggle with current fulfilment (eg. video games) vs. future fulfilment (eg. studying). Often it's better to do the latter. How do we convey this to an agent? We use what's called a \"Value Function\", $V(s)$. Thinking about how present actions will affect future rewards is called \"Delayed Rewards\".\n",
    "\n",
    "The other side of this coin is the \"Credit Assignment Problem\" - knowing what past actions contributed to a current reward. This is used in paying websites for advert clicks, as an example, for which there are multiple models. However, in RL we need a systematic and logical way of doing this. \n",
    "\n",
    "Note that values are a measure of possible future rewards - they are not the same as rewards, which are immediate.\n",
    "\n",
    "The Value Function is fast at $O(1)$, which is many magnitudes better than just evaluating all possible states in a decision tree, which grows exponentially - very bad!!!\n",
    "\n",
    "Note that some RL agents, like evolutionary/genetic algorithms, do not use Value Functions - however, a lot of RL agents do.\n",
    "\n",
    "An informal mathematical representation of the Value Function is\n",
    "$$V(s) = E \\left[ \\text{all future rewards } | \\: S(t) = s \\right] .$$\n",
    "\n",
    "This is iterative, so we just initialise $V(s)$ according to the goals. For a tic-tac-toe agent, this is: $V(s) = 1$ if in winning state, $V(s) = 0$ if in lose or draw state, $V(s) = 0.5$ if in other state.\n",
    "\n",
    "The iterative formula for finding $V(s)$ is:\n",
    "$$V(s) = V(s) + \\alpha (V(s') - V(s))$$\n",
    "where $s$ represents every state we encounter in an episode. We go through and update these iterations backwards (in reverse-chronological order) from the terminal state to the initial state*, so we obviously need to keep the states in memory. Note that this equation is very similar to both gradient descent and iteratively updating a mean, and it makes $s'$ approach the optimal values. Even though there are multiple new states $s'$, the optimal ones will have the most gravitational pull.\n",
    "\n",
    "*Based on the assumption that $V(s')$ is more accurate than $V(s)$\n",
    "\n",
    "Now that we have our Value Function the agent can use this to decide what action it will take in the current state as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxV = 0\n",
    "maxA = None\n",
    "\n",
    "for a, s_prime in possible_next_states:\n",
    "    if V(s_prime) > maxV:\n",
    "        maxV = V(s_prime)\n",
    "        maxA = a\n",
    "        \n",
    "perform action maxA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with this is that the Value Function isn't accurate - an example of the explore-exploit dilemma! Hence, as discussed in the previous chapter, we'll use Decaying Epsilon-Greedy to account for this. This enables the agent to sometimes test out new strategies to see if they're better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding the Environment and Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 agents and an environment\n",
    "\n",
    "play_game(p1, p2, env)\n",
    "\n",
    "Play_game() summary:\n",
    "<ul>\n",
    "    <li> while not over: 1) switch current player </li>\n",
    "    <li> 2) (opt.) draw board before action </li>\n",
    "    <li> 3) current player takes action on environment </li>\n",
    "    <li> both players update state history </li>\n",
    "    <li> (opt.) draw board again </li>\n",
    "    <li> update value function on environment (to query the most current reward) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
