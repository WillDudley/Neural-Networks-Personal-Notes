{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude - Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the course we will be focussing on a game we call Gridworld. In many ways this is simpler than the tic-tac-toe game investigated in last chapter.\n",
    "\n",
    "<img src=\"figures/03 - Gridworld.png\" alt=\"Gridworld\" style=\"width: 200px;\"/>\n",
    "\n",
    "The above is a visual representation of the environment. There are only eleven states and four actions (up, down, left, right) in this game, but it will allow us to explore the main features of RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sequence of RVs\n",
    "$$ \\left\\lbrace x_1, x_2, \\dotsc, x_t \\right\\rbrace $$\n",
    "we can find the probability\n",
    "$$ p \\left\\lbrace x_t \\mid x_{t-1}, x_{t-2}, \\dotsc, x_1 \\right\\rbrace .$$\n",
    "Generally, the above can't be simplified.\n",
    "\n",
    "The Markov Property specifies how many previous terms the current term depends on. The \\[first-order\\] Markov Property states that\n",
    "$$p \\left\\lbrace x_t \\mid x_{t-1}, x_{t-2}, \\dotsc, x_1 \\right\\rbrace = p \\left\\lbrace x_t \\mid x_{t-1} \\right\\rbrace ,$$\n",
    "the second-order Markov Property states that\n",
    "$$p \\left\\lbrace x_t \\mid x_{t-1}, x_{t-2}, \\dotsc, x_1 \\right\\rbrace = p \\left\\lbrace x_t \\mid x_{t-1}, x_{t-2} \\right\\rbrace ,$$\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used in the context of RL, the Markov Property states that\n",
    "$$p \\left\\lbrace S_{t+1}, R_{t+1} \\mid S_t, A_t, S_{t-1}, A_{t-1}, \\dotsc, S_0, A_0  \\right\\rbrace = p \\left\\lbrace S_{t+1}, R_{t+1} \\mid S_t, A_t  \\right\\rbrace  .$$\n",
    "This is written in notation as\n",
    "$$ p(s', r \\mid s,a) = p \\left\\lbrace S_{t+1}, R_{t+1} \\mid S_t, A_t \\right \\rbrace  .$$\n",
    "\n",
    "This is the joint probability of $s'$ and $r$, conditioned on two other variables (whereas the \"usual\" Markov Property is just one RV conditioned on one other RV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Conditional Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful conditional distributions (which can be directly derived from rules of probability) are\n",
    "$$ p (s' \\mid s,a) = \\sum_{r \\in R} p (s', r \\mid s, a)  ,$$\n",
    "$$ p (r \\mid s,a) = \\sum_{s' \\in S} p (s', r \\mid s, a)  .$$\n",
    "\n",
    "For this course and most cases, the above equations will be deterministic. This is not always the case though (eg. partially observable games)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov Decision Process is a 5-tuple of:\n",
    "<ul>\n",
    "    <li>The set of states, $\\lbrace s_i \\rbrace$</li>\n",
    "    <li>The set of actions, $\\lbrace a_i \\rbrace$</li>\n",
    "    <li>The set of rewards, $\\lbrace r_i \\rbrace$</li>\n",
    "    <li>The state-transition probabilities, $\\lbrace  p(s' \\mid s,a) \\rbrace$</li>\n",
    "    <li>The discount factor, $\\gamma$.</li>\n",
    "</ul>\n",
    "\n",
    "The other key term in the MDP is the policy, $\\pi$. While not explicitly part of the MDP itself, it helps guide actions along with the value function. There is no explicit equation for finding the policy (aside from the optimal policy which is defined in terms of the value function) - the policy is more like an algorithm, like decaying epsilon-greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Discount Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of a reward, in real may diminish over time (eg. money). This is sort-of the case in RL. The larger the number of steps an agent looks into the future, the more blurry/inaccurate the predictions get.\n",
    "\n",
    "We call the total future reward the return, $G(t)$, defined as\n",
    "$$G(t) = \\sum_{\\tau = 0}^\\infty \\gamma^\\tau R(t + 1 + \\tau) $$\n",
    "where $\\gamma$ is the Discount Factor.\n",
    "\n",
    "Usually, the Discount Factor is close to $1$, and for short episodic tasks, it may even be $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Episiodic and Continuous Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the sum goes to infinity makes the maths easier. However, this implies that the task never ends, and hence is continuous. The way to make them equivalent is to make the terminal state infinitely loop back to itself after the episode is \"over\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Functions (formal definition inc. derivation of Bellman's Eqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Value Function, given a policy $\\pi$ and a state $s$, is equal to the expected value of the return given that you are currently in state $s$:\n",
    "$$V_\\pi (s) = E_\\pi \\left[ G(t) \\mid s \\right] = E_\\pi \\left[ \\left( \\sum_{\\tau = 0}^\\infty \\gamma^\\tau R\\left( t + 1 + \\tau \\right) \\right) \\mid s \\right]  .$$\n",
    "\n",
    "Note that the Value Function depends on <b>future</b> rewards, so the Value Function at the terminal state always equals $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Value Function is recursive as\n",
    "$$\\sum_{\\tau = 0}^\\infty \\gamma^\\tau R(t+1+\\tau) = \\gamma^0 R(t+1) + \\sum_{\\tau=}^\\infty \\gamma^{\\tau+1} R(t+2+\\tau) = R(t+1) + \\gamma \\sum_{\\tau}^\\infty \\gamma^{\\tau} R(t+2+\\tau)  ,$$\n",
    "\n",
    "thus\n",
    "$$V_\\pi (s) = E_\\pi \\left[ \\left( R(t+1) + \\gamma \\sum_{\\tau}^\\infty \\gamma^{\\tau} R(t+2+\\tau) \\right) \\mid s \\right]  ,$$\n",
    "\n",
    "and so\n",
    "$$V_\\pi (s) = E_\\pi \\left[ \\left( R(t+1) + \\gamma G(t+1) \\right) \\mid s \\right]  .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Policy and Investigation of V(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V_\\pi (s)$ is the expected value over $\\pi$, so we are able to express it as a probability distribution, namely $\\pi = \\pi(a \\mid s)$. This is actually the mathematical definition of the policy $\\pi$.\n",
    "\n",
    "As the expectation is a linear operator, we are able to investigate $V_\\pi (s)$ in parts. The first part we investigate is $V_\\pi (s) = E_\\pi \\left[ R(t+1) \\mid s \\right] $. This is the expectation of $R$ given $s$. However, note that $R$ depends on $r,a,s',s$. $s$ is the only given, so we sum over all other variables (dividing by the reward as usual in expectation) as follows\n",
    "\n",
    "$$\\sum_r \\sum_{s'} \\sum_a p(r,a,s' \\mid s) r  .$$\n",
    "\n",
    "Directly from the chain rule in probability ($p(x,y \\mid z) = p(y \\mid z) p(x \\mid y,z)$), we know that \n",
    "\n",
    "$$p(r,a,s' \\mid s) = p( a \\mid s) p(r, s' \\mid s,a)  .$$\n",
    "\n",
    "The chain rule we used is derived as follows:\n",
    "\n",
    "$$p(x,y \\mid z) = \\dfrac{p(x,y,z)}{p(z)} = \\dfrac{p(x \\mid y,z) p(y,z)}{p(z)} = p(x \\mid y,z) p(y \\mid z)  .$$\n",
    "\n",
    "So we actually have\n",
    "\n",
    "$$E_\\pi \\left[ R(t+1) \\mid s \\right] = \\sum_r \\sum_{s'} \\sum_a p(r,a,s' \\mid s) r = \\sum_r \\sum_{s'} \\sum_a p( a \\mid s) p(r, s' \\mid s,a) r  .$$\n",
    "\n",
    "However, we can go further. Because we're not really interested in $r$ for specific new states $s'$, but $r$ in general (regardless of the new state), we can marginalise out $s'$. This gives us\n",
    "\n",
    "$$E_\\pi \\left[ R(t+1) \\mid s \\right] = \\sum_r \\sum_a p( a \\mid s) p(r \\mid s,a) r  ,$$\n",
    "\n",
    "which is rearranged as (for convenience):\n",
    "\n",
    "$$E_\\pi \\left[ R(t+1) \\mid s \\right] = \\sum_a \\left( p( a \\mid s) \\sum_r p(r \\mid s,a) r \\right) = \\sum_a \\left( \\pi( a \\mid s) \\sum_r r p(r \\mid s,a) \\right)  .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> Note that this expression is actually quite general:\n",
    "$$E_\\pi \\left[ (\\text{anything}) \\mid s \\right] = \\sum_a \\left( \\pi( a \\mid s) \\sum_r \\sum_{s'} (\\text{anything}) p(r,s' \\mid s,a) \\right)  .$$ </li>\n",
    "    <li> The splitting up of probability distributions has some intuition: $\\pi(s \\mid a)$ comes from \"decisions being made by the agent, whereas $p(r, s' \\mid s, a)$ comes from the environment. </li>\n",
    "    <li> $r$ can sometimes be thought of as $r(s')$ - it's the reward you get for arriving in state $s'$, in which case you can also sum over $s'$. </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued Investigation of V(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Value Function is equivalent to\n",
    "\n",
    "$$V_\\pi (s) = E_\\pi \\left[ \\left( R(t+1) + \\gamma \\sum_{\\tau}^\\infty \\gamma^{\\tau} R(t+2+\\tau) \\right) \\mid s \\right]  .$$\n",
    "\n",
    "We use the general expression for $E_\\pi \\left[ (\\text{anything}) \\mid s \\right]$ described in the previous subsection to make the step to\n",
    "\n",
    "$$V_\\pi (s) = \\sum_a \\left( \\pi( a \\mid s) \\sum_r \\sum_{s'} p(r,s' \\mid s,a) \\left\\lbrace R(t+1) + \\gamma \\sum_{\\tau}^\\infty \\gamma^{\\tau} R(t+2+\\tau) \\right\\rbrace \\right)  .$$\n",
    "\n",
    "$R(t+1)$ can just be written as $r$. Also, as $E(E(X))=E(X)$, hence $E(A+B) = E(A + E(B))$, thus $E(X) = E(E(X \\mid Y))$, we have the property \n",
    "\n",
    "$$E\\left(E\\left(G(t+1) \\mid (\\text{anything}) \\right) \\right) = E\\left( G(t+1) \\right)  .$$\n",
    "\n",
    "Here, we pick $\\text{anything}$ to be $S_{t+1} = s'$ to correspond to the new state $s'$. This brings us to\n",
    "\n",
    "$$V_\\pi (s) = \\sum_a \\left( \\pi( a \\mid s) \\sum_r \\sum_{s'} p(r,s' \\mid s,a) \\left\\lbrace r + \\gamma E_\\pi \\left[ \\sum_{\\tau}^\\infty \\gamma^{\\tau} R(t+2+\\tau) \\mid S_{t+1} = s' \\right] \\right\\rbrace \\right)  .$$\n",
    "\n",
    "So, by definition of $V_\\pi (s)$, this is equivalent to\n",
    "\n",
    "$$V_\\pi (s) = \\sum_a \\left( \\pi( a \\mid s) \\sum_r \\sum_{s'} p(r,s' \\mid s,a) \\left\\lbrace r + \\gamma V_\\pi(s') \\right\\rbrace \\right)  .$$\n",
    "\n",
    "This shows us formally the recursiveness of the Value Function. It is important to note that $V_\\pi(s')$ encapsulates all knowledge about the future, so in order to look into the future from $s$, we only need to look at $V_\\pi(s')$ - just one step ahead!\n",
    "\n",
    "The above equation is very important in RL. All the algorithms we're going to investigate are based on this equation. This equation is named \"The Bellman Equation\", named after Richard Bellman (who pioneered dynamic programming, a popular technique which can help solve MDPs). The equation is efficient because it builds up using a \"bottom-up\" approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The State-Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've just learned about is called the State-Value Function,\n",
    "\n",
    "$$V_\\pi (s) = E_\\pi \\left[ G(t) \\mid S_t = s \\right]  .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Action-Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another value function, called the Action-Value Function,\n",
    "\n",
    "$$Q_\\pi (s,a) = E_\\pi \\left[ G(t) \\mid S_t = s, A_t = a \\right]  .$$\n",
    "\n",
    "$Q_\\pi (s,a)$ is called the Action-Value Function because the action is also a parameter of the function. For this function, the space complexity is quadratic due to having two inputs $s$ and $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which do we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function we use depends on the algorithm that we're using. There exist some algorithms that only apply when we use the State-Value Function, and some others that only apply when we use the Action-Value Function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of this course is pretty much devoted to finding solutions to the Bellman Equation - this is a core concept of RL. There are two ways to approach finding a solution to the Bellman Equation, as described here onwards. We will go through examples 5-7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First way - Iterating Backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following diagram with probabilities labelled on arrows and rewards labelled on nodes\n",
    "\n",
    "<img src=\"figures/03 - example-5.png\" alt=\"Example 5\" style=\"width: 200px;\"/>\n",
    "\n",
    "with discount factor $\\gamma = 0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two terminal states, where all the non-zero rewards are situated. Thus we have\n",
    "$$V(S4) = 0 , \\qquad V(S5)=0 ,$$\n",
    "\n",
    "$$V(S2) = 0.8 \\times -1 + 0.2 \\times 1 = -0.6  ,$$\n",
    "\n",
    "$$V(S3) = 0.1 \\times -1 + 0.9 \\times 1 = 0.8  ,$$\n",
    "\n",
    "$$V(S1) = 0.5 \\times (0 + \\gamma V(S2)) + 0.5 \\times (0 + \\gamma V(S3))$$\n",
    "$$ = 0.5 \\times 0.9 \\times -0.6 + 0.5 \\times 0.9 \\times 0.8 = 0.09  .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6 - Difference between States and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more realistic example. Consider playing a game of dodgeball. There are two states, \"not hit\" (safe) and \"hit\", as shown in the diagram below.\n",
    "\n",
    "<img src=\"figures/03 - example-6.png\" alt=\"Example 6\" style=\"width: 200px;\"/>\n",
    "\n",
    "However, it doesn't really make sense to call the actions \"not hit\" and \"hit\" - if we could simply decide not to get hit then we'd be very good at dodgeball!. A more sensible decision is to call the actions, for example, \"duck\" or \"jump\".\n",
    "\n",
    "Then, for example, we could have the following policy and conditional probabilities:\n",
    "\n",
    "<img src=\"figures/03 - example-6-2.png\" alt=\"Example 6 Probabilities\" style=\"width: 300px;\"/>\n",
    "\n",
    "Since the rewards are deterministic (they completely depend on the state), we can marginalise over the reward to get\n",
    "\n",
    "<img src=\"figures/03 - example-6-3.png\" alt=\"Example 6 Probabilities 2\" style=\"width: 200px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the Bellman Equation to calculate $V_\\pi (\\textit{start})$. As both future rewards are $0$ ($V(s)=0$ for terminal state $s$), $V(s')=0$. Hence we have\n",
    "\n",
    "$$V_\\pi (\\textit{start}) = \\sum_{a \\in \\textit{ duck, jump}} \\left( \\pi( a \\mid s) \\sum_{s' \\in \\textit{ safe, hit}} p(s' \\mid s,a) r(s,s') \\right)  ,$$\n",
    "\n",
    "so\n",
    "\n",
    "$$V_\\pi (\\textit{start}) = \\pi (\\textit{jump}) p(\\textit{safe} \\mid \\textit{jump}) \\times 0 + \\pi (\\textit{jump}) p(\\textit{hit} \\mid \\textit{jump}) \\times (-1) + \\pi (\\textit{duck}) p (\\textit{safe} \\mid \\textit{duck}) \\times 0 + \\pi (\\textit{duck}) p(\\textit{hit} \\mid \\textit{duck}) \\times (-1)  $$\n",
    "$$= (0.5)(0.8)(-1) + (0.5)(0.4)(-1) = -0.6  \\: .$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second way - Solving Linear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7 - Added Complexity of Cycles (Loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following diagram\n",
    "\n",
    "<img src=\"figures/03 - example-7.png\" alt=\"Example 7\" style=\"width: 400px;\"/>\n",
    "\n",
    "with discount factor $\\gamma = 0.9$.\n",
    "\n",
    "This is slightly more complex as we deal with cycles - so there's no clear notion of going \"backwards\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply Bellman's Equation to all states and see if we can work from there. Let's start with $V(S_1)$:\n",
    "\n",
    "$$V(S_1) = (0.3)(-0.1 + 0.9V(S_1)) + (0.7)(-0.1 + 0.9V(S_2))  ,$$\n",
    "so\n",
    "$$V(S_1) = -0.1 + 0.27V(S_1) + 0.63V(S_2)  .$$\n",
    "\n",
    "Now let's do $V(S_2)$:\n",
    "\n",
    "$$V(S_2) = (0.6)(-0.1 + 0.9V(S_1)) + (0.4)(1 + 0.9V(S_3))  ,$$\n",
    "so\n",
    "$$V(S_2) = 0.34 + 0.54V(S_1) + 0.36 V(S_3)  .$$\n",
    "\n",
    "Finally, $V(S_3) = 0$ as $S_3$ is a terminal state.\n",
    "\n",
    "Here, we have three equations and 3 unknowns. Hence we have\n",
    "$$\\begin{pmatrix}\n",
    "-0.73 & 0.63 & 0 \\\\\n",
    "0.54 & -1 & 0.36 \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "V(S_1) \\\\\n",
    "V(S_2) \\\\\n",
    "V(S_3) \n",
    "\\end{pmatrix}  \n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0.1 \\\\\n",
    "-0.34 \\\\\n",
    "0\n",
    "\\end{pmatrix}  .$$\n",
    "\n",
    "If doing this in code, one should use the following, which is more efficient than explicitly using the inverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29297075]\n",
      " [0.49820421]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.matrix([[-0.73, 0.63, 0], [0.54, -1, 0.36], [0, 0, 1]])\n",
    "b = np.matrix([[0.1], [-0.34], [0]])\n",
    "\n",
    "x = np.linalg.solve(A, b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Policies and Optimal Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two concepts are key in RL (lots of depth to them), and they are interdependent.\n",
    "\n",
    "We say that the policy $\\pi_1$ is better than the policy $\\pi_2$ ($\\pi_1 \\geq \\pi_2$) if the following condition holds:\n",
    "\n",
    "$$V_{\\pi_1} (s) \\geq V_{\\pi_2} (s)  \\quad \\forall s \\in S  .$$\n",
    "\n",
    "The Optimal Policy, $\\pi^*$ is the policy corresponding to the Optimal Value Function, defined as\n",
    "\n",
    "$$V^* (s) = \\max_\\pi \\left(V_\\pi (s) \\right)  \\quad \\forall s \\in S  .$$\n",
    "\n",
    "Optimal Policies are not necessarily unique. However, Optimal Value Functions are.\n",
    "\n",
    "Similarly, the Optimal Action-Value Function is defined as\n",
    "\n",
    "$$Q^* (s,a) = \\max_\\pi \\left(Q_\\pi (s,a) \\right)  \\quad \\forall s \\in S, a \\in A  ,$$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$Q^* (s,a) = E \\left[ R(t+1) + \\gamma V^* (S_{t+1}) \\mid S_t = s, A_t = a \\right]  .$$\n",
    "\n",
    "Also, we have\n",
    "\n",
    "$$V^* (s) = \\max_a \\left( Q^* (s,a) \\right)  .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bellman Optimality Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we directly know that\n",
    "\n",
    "$$V^* (s) = \\max_a E \\left[ R(t+1) + \\gamma V^* (S_{t+1}) \\mid S_t = s, A_t = a \\right]  ,$$\n",
    "\n",
    "so\n",
    "\n",
    "$$V^* (s) = \\max_a \\sum_r \\sum_{s'} p(r,s' \\mid s,a) \\left[ r + \\gamma V^*s') \\right]  .$$\n",
    "\n",
    "This is known as the Bellman Optimality Equation for the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, there exists the Bellman Optimality Equation for the action-value function:\n",
    "\n",
    "$$Q^* (s,a) = E \\left[ R(t+1) + \\gamma \\max_{a'} Q^* (S_{t+1},a') \\mid S_t = s, A_t = a \\right]  ,$$\n",
    "\n",
    "so\n",
    "\n",
    "$$Q^* (s,a) = \\sum_r \\sum_{s'} p(r,s' \\mid s,a) \\left[ r + \\gamma \\max_{a'} Q^*(s',a') \\right]  .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the value function already takes into account future rewards, nothing extra needs to be done to optimise the future rewards. All that we need to do is choose the action that yields the best next-state value function $V(s')$. \n",
    "\n",
    "For this, we would need to consider all potential actions $a$ (perform a look-ahead search). If we have $Q(s,a)$, this will essentially act as a cache for the look-ahead search results, hence in this case we can just choose the argmax of $Q(s,a)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
