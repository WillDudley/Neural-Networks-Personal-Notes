{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter will focus on iterative policy evaluation. This iteratively finds the optimal policy for a task by applying the Bellman Equation repeatedly, and convergence will follow. The problem of finding $V(s)$ given a policy is called the \"prediction problem\". The problem of finding the optimal policy is called the \"control problem\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See \"Gridworld_in_code.py\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Environment class</b>\n",
    "<ul>\n",
    "    <li>Needs width, height, start_pos attribs. start_pos split into x coord and y coord.</li>\n",
    "    <li>Needs function to set tile_reward dictionary and possible_actions dictionary as attribs.</li>\n",
    "    <li>Needs function to set pos to an inputted tuple.</li>\n",
    "    <li>Good to have function that returns current_pos as tuple.</li>\n",
    "    <li>Good to have function is_in_terminal that returns T/F.</li>\n",
    "    <li>Needs function to execute actions from the state's possible_actions. Also returns reward of new state (hint: use rewards.get() ).</li>\n",
    "    <li>Undo move?</li>\n",
    "    <li>Needs function to return pos if game is over (hint: is pos in possible_actions?)</li>\n",
    "    <li>Needs function to get all_states</li>\n",
    "</ul>\n",
    "\n",
    "<b>standard_grid()</b>\n",
    "<ul>\n",
    "    <li>Describes rewards and possible actions at each state.</li>\n",
    "    <li>Returns a set of rewards and actions</li>\n",
    "</ul>\n",
    "\n",
    "<b>negative_grid(step_cost=-0.1)</b>\n",
    "<ul>\n",
    "    <li>Returns dictionary with all possible non-terminal positions being assigned the step_cost.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Policy Evaluation in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how we can find the value function for different policies, we'll do Iterative Policy Evaluation on two different policies.\n",
    "\n",
    "The first policy we'll look at is a completely random uniform policy. For this we'll just focus on $\\pi(a \\mid s)$, as $p(s', r \\mid s,a)$ is only relevant when state transitions are random.\n",
    "\n",
    "The second policy is a completely deterministic policy where we go towards the positive terminal state if we're along the top or left side, else we go towards the negative terminal state.\n",
    "\n",
    "See \"Iterative_Policy_Evaluation_in_code.py\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our previous file import standard grid\n",
    "\n",
    "<b>print_values(V, g)</b>\n",
    "\n",
    "For i in rows print a line, for j in columns print $V(s)$ |.\n",
    "\n",
    "<b>print_policy(P, g)</b>\n",
    "\n",
    "Same as print_values() but with $\\pi(s)$. Only works for deterministic policies as only room for one value per state.\n",
    "\n",
    "<b>if __name__ == '__main__':</b>\n",
    "<ul>\n",
    "    <li>initialiise standard_grid()</li>\n",
    "    <li>initialise all_states()</li>\n",
    "    <li>For first policy: see code</li>\n",
    "    <li>For second policy: explicitly define the policy and follow similar logic</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem of how to find better policies (and ultimately the optimal policy) is called \"the control problem\". We've already seen how to find the value function ($V$ and $Q$) given a fixed policy.\n",
    "\n",
    "Note that\n",
    "\n",
    "$$Q_\\pi (s,a) = \\sum_{s'} \\sum_{r} p(s',r \\mid s,a) \\left[ r + V_\\pi (s') \\right]  ,$$\n",
    "\n",
    "so we can say\n",
    "\n",
    "$$V_\\pi(s) = Q_\\pi(s, \\pi(s)) = \\sum_{s'} \\sum_{r} p(s',r \\mid s,\\pi(s)) \\left[ r + V_\\pi (s') \\right]  .$$\n",
    "\n",
    "If we want to, we can change one of the actions in the policy. We do this in order to find a better policy, ie.\n",
    "\n",
    "$$\\text{find } a \\in A : Q_\\pi(s,a) > Q_\\pi(s,\\pi(s))  .$$\n",
    "\n",
    "Intuitively, all this is saying is \"if the current policy is to perform action $a$, check the other actions to see if this improves the value function - if so, update the policy with this new action\".\n",
    "\n",
    "Formally speaking, we're finding a new policy $\\pi'$ such that\n",
    "$$V_\\pi(s) \\leq V_{\\pi'}(s)  ,$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\pi'(s)=\\textit{argmax}_aQ_\\pi(s,a)=\\textit{argmax}_a\\sum_{s'} \\sum_{r} p(s',r \\mid s,a) \\left[ r + V_\\pi (s') \\right]  .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Policy Improvement is greedy and only looks at the current state, it never considers the value function at all states.</li>\n",
    "    <li>Policy Imporovement uses an imperfect version of the value function - once we change $\\pi$, $V_\\pi(s)$ also changes. This actually is not a problem for reasons seen in the next section.</li>\n",
    "    <li>When we've found the optimal policy, the value function's output doesn't change when put through Policy Improvement.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As just mentioned, we can find the optimal policy via iteration. A potential problem we realised was once we change the policy, the value function becomes out of date. \n",
    "\n",
    "To resolve this, we just recalculate the value function given the new policy. We already know how to do this from the section \"Iterative Policy Evaluation in Code\". The trick here is to alternate between policy evaluation and policy improvement until the policy doesn't change. Note that it is no longer needed to check for convergence, and instead check for equality.\n",
    "\n",
    "The steps of Policy Iteration are as follows:\n",
    "<ol>\n",
    "    <li>Randomly initialise $V(s)$ and $\\pi(s)$.</li>\n",
    "    <li>Set $V(s)$ = iterative_policy_evaluation($\\pi$).</li>\n",
    "    <li>Until policy hasn't changed, for $s$ in all states calculate $\\pi(s)$ (using the concluding equation from last section).</li>\n",
    "</ol>\n",
    "\n",
    "See \"Policy_Iteration_in_code.py\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our previous files import standard_environment (or negative_environment if you want), and print_values, print_policy.\n",
    "\n",
    "<b>if __name__ == '__main__':</b>\n",
    "<ul>\n",
    "    <li>init env</li>\n",
    "    <li>print values</li>\n",
    "    <li>init random policy and print</li>\n",
    "    <li>init V</li>\n",
    "    <li>evaluation-improvement alternation - see code</li>\n",
    "    <li>print values and policy</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration in Windy Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the environment ($p(s',r \\mid s,a)$) has been deterministic. In windy gridworld, this isn't the case.\n",
    "\n",
    "In windy gridworld, there's an RNG aspect after inputting an action. The probability of successfully going in the desired direction (eg. \"up\") is $0.5$, and the probability of going in another direction (\"left\", \"down\", \"right\") is $\\dfrac{0.5}{3}$.\n",
    "\n",
    "See \"Policy_Iteration_Random_in_code.py\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences with \"Policy_Iteration_in_code.py\" are as follows:\n",
    "<ul>\n",
    "    <li>Step cost is now 1.0. Due to this, if things aren't going as planned it'd just be better to take the loss.</li>\n",
    "    <li>In the policy evaluation step, we loop through all four actions and if the action agrees with the policy at the state, p = 0.5 else p = 0.5/3. Similar with policy improvement. See code. Remember to sum over all actions.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration is a valid way to solve the control problem. However, it does involve an iterative algorithm inside of another iterative algorithm (very expensive). It'd be worthwhile to see if there's a more efficient way of solving  the control problem.\n",
    "\n",
    "The key thing to note is that there is actually a point before $V$ converges where the resulting greedy policy wouldn't change, essentially making all computations past this point unnecessary. Just a few steps of policy evaluation are actually necessary for policy improvement to find the same policy.\n",
    "\n",
    "Value Iteration takes this a step further, combining policy evaluation and policy improvement into one step:\n",
    "\n",
    "$$V_{k+1}(s)=\\max_a\\sum_{s'}\\sum_rp(s',r\\mid s,a)\\left\\lbrace r+\\gamma V_k(s') \\right\\rbrace  .$$\n",
    "\n",
    "This equation is very similar to the policy evaluation equation except we're taking the max over all actions. It's still iterative, but we no longer need to wait for iteration $k$ to finish before calculating iteration $k+1$ - we can just update it in-place. Since policy improvement uses argmax, by taking the max, we're just doing the next policy evaluation step without calculating the policy explicitly.\n",
    "\n",
    "See \"Value_Iteration_in_code.py\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we go back to deterministic gridworld.\n",
    "\n",
    "Policy evaluation is the same aside from the fact that we loop through all possible actions and take the maximum value.\n",
    "\n",
    "After that, we take the optimal value function and find the optimal policy. So for every state in actions we loop through all actions and use the deterministic Bellman's equation to find the best action (the action that's associated with the highest possible future reward for that state).\n",
    "\n",
    "See code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter discussed Dynamic Programming - one method for finding solutions to a MDP. Iterative policy evaluation was used to solve the prediction problem (finding the value function given a policy). Policy iteration and later value iteration were both used to solve the control problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every algorithm we discussed involved looping through all states - a rediculously large task for most games. Value iteration helps this by updating $V(s)$ in-place, but Asynchronus Dynamic Programming takes this a state further: Looping through only a few states per iteration. The states to look at can either be chosen randomly or chosen based on which states are the most-visited (which can be learned through playing the game)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main concept behind policy iteration (as we've seen) is that we iteratively alternate between policy evaluation and policy improvement until convergence. Convergence occurs when Bellman's equation becomes true for the value function.\n",
    "\n",
    "<img src=\"figures/04 - policy-iteration.png\" alt=\"Example 7\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency of Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar as in the tic-tac-toe chapter, the time complexity of dynamic programming is $O(M^N)$, where $M=$ number of actions, and $N=$ number of states. If you were to do this manually, you'd list out all permutations of possible state sequences, do policy evaluation on all of them and keep the policy that gives the highest value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based vs. Model-free and Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how DP requires a full model of the environment (it's model-based), in particular the state-transition probabilities $p(s',r \\mid s,a)$. As mentioned previously, the notion of having to deal with a whole model of the environment can get extremely large. The remaining chapters of this course will look at model-free methods which don't have this requirement.\n",
    "\n",
    "Furthermore, note how these iterative methods require an initial estimate (making an initial estimate is called bootstrapping) because we generate an estimate from a previous estimate. Monte Carlo does not require bootstrapping, however Temporal difference does."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
