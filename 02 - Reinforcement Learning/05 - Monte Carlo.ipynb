{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the last chapter, we concluded that DP was model-based. This means that it didn't need to actually play any episodes - we were in \"God-mode\". However, for many scenarios such as self-driving cars and video-games, it's unreasonable to think that you'd know everything about the environment. Monte Carlo methods on the other hand learn puely from experience.\n",
    "\n",
    "Monte Carlo usually refers to any method with a significant random component. In RL, the random component is the return. With MC we don't calculate the true expected value of the return ($G$) - instead we calculate its sample mean.\n",
    "\n",
    "Before we can calculate returns, the episode must terminate, hence MC only works with episodic tasks. Also due to this, MC isn't \"fully online\" ie. improvements happen after episodes rather than in real time (after every action).\n",
    "\n",
    "This is similar to the multi-armed bandits of Chapter 1. With that, we'd always average the reward after every episode. With MDPs we're always averaging the return.\n",
    "\n",
    "We'll follow the same pattern as last chapter, first investigating the prediction problem, then moving on to the control problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that\n",
    "\n",
    "$$V_\\pi(s) = E \\left[ G(t) \\mid S_t=s \\right]  .$$\n",
    "\n",
    "Also note that for $i=$ episode and $s=$ state,\n",
    "\n",
    "$$\\overline{V}_\\pi(s)=\\dfrac{1}{N}\\sum_{i=1}^NG_{i,s}  .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate $G$, we just play a number of episodes, record the states and reward sequences. Then we can calculate $G$ from the definition (iterating through the states in reverse order since $G$ depends only on future values),\n",
    "\n",
    "$$G(t) = r(t+1) + \\gamma\\left( G(t+1) \\right)  .$$\n",
    "\n",
    "Once we have the pairs $(s,G)$, we just average them for each $s$ (ie. take the sample mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple visits to $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we see a state $s$ more than once in an episode, ie. we see $s$ at $t=1$ and $t=3$, we can either only include the first instance in our sample mean (first-visit MC), or include them all (every-visit MC). It has been proven that they both lead to the same answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_prediction(policy, N):\n",
    "    \n",
    "    V = random_initialisation\n",
    "    all_returns = {}\n",
    "    \n",
    "    do N times:\n",
    "        states, returns = play_episode\n",
    "        \n",
    "        for s, g in zip(states,returns):\n",
    "            if not seen s this episode yet:\n",
    "                all_returns[s].append(g)\n",
    "                V(s) = sample_mean(all_returns[s])\n",
    "                \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Mean Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in Chapter 1 we discussed more efficient ways of calculating the sample mean. Everything learnt there can be applied here.\n",
    "\n",
    "The rules of probability still apply:\n",
    "<ul>\n",
    "    <li>The confidence interval is approximately Gaussian (Central Limit Theorem).</li>\n",
    "    <li>The variance is the original variance of the data divided by the number of samples collected. Hence we are going to be more confident the more samples we have, but growth gets slower the more we have.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Returns from Rewards - Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.return_pos()\n",
    "\n",
    "states_and_rewards = [(state, 0)]\n",
    "while not env.game_over:\n",
    "    action = policy(state)\n",
    "    reward = env.move(action)\n",
    "    state = env.return_pos()\n",
    "    states_and_rewards.append((s, r))\n",
    "\n",
    "G = 0\n",
    "states_and_returns = []\n",
    "for state, reward in reverse(states_and_rewards):\n",
    "    states_and_returns.append((state, G))\n",
    "    G = reward + GAMMA * G\n",
    "\n",
    "states_and_returns.reverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Advantage of MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that one disadvantage of DP is that we need to loop through all states. For MC, we only update $V$ for visited states - we don't need to know what all the states are, we can just discover them as we play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
